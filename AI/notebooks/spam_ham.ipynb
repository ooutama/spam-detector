{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2859a1de"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brief-damages"
      },
      "outputs": [],
      "source": [
        "import re\r\n",
        "import string\r\n",
        "import csv\r\n",
        "from joblib import dump, load\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "#pd.set_option('display.max_colwidth', None)\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "import plotly.express as px\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk import SnowballStemmer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "\r\n",
        "from wordcloud import WordCloud\r\n",
        "\r\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "entertaining-clone"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apparent-annotation"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../data/SMSSpamCollection.txt', sep=',\\s+',quoting=csv.QUOTE_ALL, delimiter='\\t', names= ['target','message'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed48e51a"
      },
      "outputs": [],
      "source": [
        "df['target'] = df['target'].apply(lambda row: row.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15a82d49"
      },
      "source": [
        "#### Replace special characters (`&gt;` , `&lt;`, `<DECIMAL>`...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0f70cf8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "special_chars = {\n",
        "    \"&quot;\" : '\"',\n",
        "    \"&apos;\" : \"'\",\n",
        "    \"&amp;\" : \"&\",\n",
        "    \"&lt;\" : \"<\",\n",
        "    \"&gt;\" : \">\"\n",
        "}\n",
        "df = df.replace({'message': special_chars}, regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a2d3435",
        "outputId": "6bafb05a-f9f1-489e-dc9b-4b0eff4f6ffc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "brackets = set()\n",
        "def find_brackets(row):\n",
        "    global brackets\n",
        "    for bracket in re.findall(r'<[a-zA-Z0-9\\s#]+>+', row):\n",
        "        brackets.add(bracket)\n",
        "\n",
        "df['message'].apply(find_brackets)\n",
        "print(brackets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2af72e1e"
      },
      "outputs": [],
      "source": [
        "special_chars = {\n",
        "    \"<DECIMAL>\" : '123',\n",
        "    \"<UKP>\" : \"£\",\n",
        "    \"<EMAIL>\" : \"abc@email.com\",\n",
        "    \"<fone no>\" : \"<phone 0123456789>\",\n",
        "    \"<URL>\" : \"https://www.abcd.com\",\n",
        "    \"<#>\" : \"321\",\n",
        "    \"<TIME>\" : \"13:00\"\n",
        "}\n",
        "df = df.replace({'message': special_chars}, regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5a8834e",
        "outputId": "f8e8dd18-308c-4309-99fb-0bcbcc77bdb9"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "258b2cc0"
      },
      "source": [
        "- Our dataset contains 5574 observation and 2 characterisitics (which one is dependant variable, the other one is an independant variable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff5a8589",
        "outputId": "14d44d64-58a1-471f-c8f3-ed5a1477a855"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7af68c16"
      },
      "source": [
        "- The dataset consists of 5574 English messages, each entry is designated as being ham or spam. Dataframe has two columns: \n",
        " - The first column `target` indicating the class of message as ham or spam.\n",
        " - The second column `message` is the content of the message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "b1d7a70d",
        "outputId": "7f877c61-5638-4546-d7d4-2e3fa9f70899",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df.sample(10, random_state=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "986e3142",
        "outputId": "d3023fb4-7b3e-4269-9fc4-d52baad15416",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df.groupby('target').describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a70da187"
      },
      "source": [
        "- Our dataset contains 4518 (out of 4827) unique `ham` messages and 653 (out of 747) unique `spam` ones.\n",
        "- `Sorry, I'll call later` is the most common `ham` message.\n",
        "- `Please call our customer service representative on FREEPHONE...` is the most common `spam` message.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3c5eb15"
      },
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c3da38e"
      },
      "source": [
        "We will be adding the following columns to the dataframe:\n",
        "- `word_count` : The number of words\n",
        "- `sentence_count` : The number of sentences\n",
        "- `brackets_count` : The number of text between brackets\n",
        "- `links_count` : The number of links\n",
        "- `phone_count` : The number of phone numbers\n",
        "- `money_count` : The number of money amounts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21f23b59",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class FeaturesExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "      nltk.download('punkt', quiet=True)\n",
        "    \n",
        "    def count_words(self, input_text):\n",
        "        # remove punctuation, tokenize and return the number of tokens (words)\n",
        "        message = input_text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return len(nltk.word_tokenize(input_text))\n",
        "\n",
        "    def count_sentences(self, input_text):\n",
        "        return len(sent_tokenize(input_text.lower()))\n",
        "\n",
        "    def count_brackets(self, input_text):\n",
        "        return len(re.findall(r'<[a-zA-Z0-9\\s]+>+', input_text.lower()))\n",
        "\n",
        "    def count_links(self, input_text):\n",
        "        return len(re.findall(r'https?://\\S+|www\\.\\S+', input_text.lower()))\n",
        "\n",
        "    def count_phone(self, input_text):\n",
        "        return len(re.findall(r'\\d{5,}', input_text.lower()))\n",
        "\n",
        "    def count_money(self, input_text):\n",
        "        return len(re.findall(r'[$|£|€]\\d+', input_text.lower()))+len(re.findall(r'\\d+[$|£|€]', input_text.lower()))\n",
        "\n",
        "    def transform(self, df, y=None):\n",
        "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
        "        df['word_count'] = df.message.apply(self.count_words)\n",
        "        df['sentence_count'] = df.message.apply(self.count_sentences)\n",
        "        df['brackets_count'] = df.message.apply(self.count_brackets)\n",
        "        df['links_count'] = df.message.apply(self.count_links)\n",
        "        df['phone_count'] = df.message.apply(self.count_phone)\n",
        "        df['money_count'] = df.message.apply(self.count_money)\n",
        "        return df\n",
        "    def fit(self, df, y=None):\n",
        "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
        "        return self\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "5dfdba10",
        "outputId": "92dbb40a-2f05-4d7b-ead5-177d2cca1591"
      },
      "outputs": [],
      "source": [
        "tc = FeaturesExtractor()\n",
        "\n",
        "df = tc.fit_transform(df)\n",
        "\n",
        "df.sample(10, random_state=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "other-jumping"
      },
      "source": [
        "# EDA (Exploratory data analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "29c063d6",
        "outputId": "3bb1bf0d-06f4-4166-fd76-50a8ec8d22b8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76e08cba"
      },
      "source": [
        "- The average number of words is (`mean`) 18 words\n",
        "- The average number of sentences is (`mean`) 2 sentences\n",
        "- The mean values are close to medians\n",
        "- There's a large difference  between the 75% and max value (there might be outliers in the dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb1f1ad8"
      },
      "source": [
        "- The longest message (in terms of `word_count`) contains 196 (`max`) words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "83f05994",
        "outputId": "f2a6655f-d465-482e-af26-f507e37048d8",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "df.loc[df['word_count'] == df['word_count'].max()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bda8cf7"
      },
      "source": [
        "- The shortest message contains 1 word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4e20c0dc",
        "outputId": "fb771e5e-1cc8-48b5-b187-2849770efb2c",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "df.loc[df['word_count'] < 2].sort_values('word_count').head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dffcb810"
      },
      "source": [
        "## Detecting outliers using the 1.5 IQR (Interquartile Rule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f651b3f7"
      },
      "source": [
        "### Applying the 1.5 IQ rule to `word_count`\n",
        "- IQR = Q3-Q1 = 16 | Q1 = 7 | Q3 = 23\n",
        "- IQR * 1.5 = 24\n",
        "- IQR * 1.5 - Q1 = 17\n",
        "- Q3 + IQR * 1.5 = 47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a35aa29",
        "outputId": "fca49755-79b0-4b7c-d765-f8a04f8ba0d5"
      },
      "outputs": [],
      "source": [
        "wc_desc = df.describe()['word_count']\n",
        "wc_Q1, wc_Q3 = wc_desc['25%'], wc_desc['75%']\n",
        "IQR = wc_Q3 - wc_Q1\n",
        "low, high = IQR * 1.5 - wc_Q1 , wc_Q3 + IQR * 1.5\n",
        "print(f'Low outliers count : {len(df[df.word_count<low])}\\nHigh outliers count : {len(df[df.word_count>high])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "320c094f"
      },
      "source": [
        "- Removing low outliers will result in removing more than **half** of the data, while high outliers count is low so it will not affect our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc8226c2"
      },
      "source": [
        "### Applying the 1.5 IQ rule to `sentence_count`\n",
        "- IQR = Q3-Q1 = 16 | Q1 = 7 | Q3 = 23\n",
        "- IQR * 1.5 = 24\n",
        "- IQR * 1.5 - Q1 = 17\n",
        "- Q3 + IQR * 1.5 = 47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c01ae0e5",
        "outputId": "e33556df-fc51-4d8a-ad66-ec2cf61247cc"
      },
      "outputs": [],
      "source": [
        "wc_desc = df.describe()['sentence_count']\n",
        "wc_Q1, wc_Q3 = wc_desc['25%'], wc_desc['75%']\n",
        "IQR = wc_Q3 - wc_Q1\n",
        "low, high = IQR * 1.5 - wc_Q1 , wc_Q3 + IQR * 1.5\n",
        "print(f'Low outliers count : {len(df[df.sentence_count<low])}\\nHigh outliers count : {len(df[df.sentence_count>high])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88638786"
      },
      "source": [
        "- There are no lower outliers in the dataset, while there're 662 high outliers. Like `word_count`, high outliers count is low so they will not affect the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72c1acfd"
      },
      "source": [
        "### Pairplot of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dd96b237",
        "outputId": "36ae710a-617c-4ab6-8f14-970c1af8c107",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_matrix(df,\n",
        "                        dimensions=df.drop(['target','message'],axis=1).columns,\n",
        "                        color=\"target\",\n",
        "                        height=700)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flying-secondary"
      },
      "source": [
        "## Count of every class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "assumed-dance",
        "outputId": "8b374980-9814-458e-ec79-11f85a3980ea",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "counts = df.groupby('target').count().reset_index()\n",
        "counts = counts.rename(columns = {\"message\":\"count\"})\n",
        "fig = px.bar(counts,\n",
        "             x='target',\n",
        "             y='count',\n",
        "             color='target',\n",
        "             width = 500, height=400)\n",
        "fig.update_layout(title_text='Count of ham and spam messages in the dataset',\n",
        "                  xaxis_title_text='Class',\n",
        "                  yaxis_title_text='Count')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f641d1d"
      },
      "source": [
        "- The dataset contains **4827** `ham` messages, and **747** `spam` ones\n",
        "- The dataset is imbalanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e5d0663"
      },
      "source": [
        "## Word count distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "eligible-haiti",
        "outputId": "c1a6e47f-5f8e-483b-8e42-73d5b8171152",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fig_hist = px.histogram(df,\n",
        "                        x=\"word_count\",\n",
        "                        color=\"target\",\n",
        "                        barmode=\"overlay\",\n",
        "                        marginal=\"violin\", #box or rug\n",
        "                        hover_data=df.columns,\n",
        "                        range_x=[\"0\",\"100\"],\n",
        "                        width=700)\n",
        "fig_hist.update_layout(title_text='Message length distribution (Word count)',\n",
        "                       xaxis_title_text='Word Count',\n",
        "                       bargap=0.3)\n",
        "fig_hist.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d46a16b"
      },
      "source": [
        "- As we can see, `ham` messages lenght tends to be lower than `spam` messages lenght."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f811a28a"
      },
      "source": [
        "## Sentence count distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "bf336832",
        "outputId": "ab1c4d0f-c0d5-4c52-f90e-5352b87fd739",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "fig_hist = px.histogram(df,\n",
        "                        x=\"sentence_count\",\n",
        "                        color=\"target\",\n",
        "                        barmode=\"overlay\",\n",
        "                        marginal=\"violin\", #box or rug\n",
        "                        hover_data=df.columns,\n",
        "                        range_x=[\"0\",\"40\"],\n",
        "                        width=700)\n",
        "fig_hist.update_layout(title_text='Message length distribution (Sentence count)',\n",
        "                       xaxis_title_text='Sentence Count',\n",
        "                       bargap=0.3)\n",
        "fig_hist.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "political-feature"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZqOE6fcmJgF"
      },
      "outputs": [],
      "source": [
        "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self, cols):\n",
        "        self.cols = cols\n",
        "\n",
        "    def transform(self, X, **transform_params):        \n",
        "        return X[self.cols]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b86df83"
      },
      "source": [
        "### Text cleaning\r\n",
        "\r\n",
        "Before we start using the messages we clean them. We'll do the this in the class CleanText:\r\n",
        "\r\n",
        "    - Set all words to lowercase\r\n",
        "\t- Replace text between brackets with 'bracketstext' (present in spam messages)\r\n",
        "\t- Replace money amounts ($123 or 1£) with 'moneytext'\r\n",
        "\t- Replace remaining currency symbols with 'currsymb'\r\n",
        "\t- Replace links with 'weblink'\r\n",
        "\t- Replace phone numbers with 'phonenumber'\r\n",
        "\t- Replace punctuation with whitespace\r\n",
        "\t- Remove extra whitespaces\r\n",
        "    - Remove remaining digits\r\n",
        "    - Remove stopwords\r\n",
        "    - Apply SnowballStemmer to keep the stem of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55209637"
      },
      "outputs": [],
      "source": [
        "class CleanText(BaseEstimator, TransformerMixin):\r\n",
        "\r\n",
        "\tdef __init__(self):\r\n",
        "\t\tnltk.download('stopwords', quiet=True)\r\n",
        "\t\tnltk.download('wordnet', quiet=True)\r\n",
        "\t\r\n",
        "\tdef to_lower(self, input_text):\r\n",
        "\t\treturn input_text.lower()\r\n",
        "\r\n",
        "\tdef replace_brackets(self, input_text):\r\n",
        "\t\t# Replace text between brackets with 'bracketstext' (spam messages)\r\n",
        "\t\treturn re.sub('<.*?>+', ' bracketstext ', input_text)\r\n",
        "\r\n",
        "\tdef replace_money(self, input_text):\r\n",
        "\t\t# Replace money amounts ($123 or 1£) with 'moneytext'\r\n",
        "\t\tinput_text = re.sub(r'[$|£|€]\\d+', ' moneytext ', input_text)\r\n",
        "\t\treturn re.sub(r'\\d+[$|£|€]', ' moneytext ', input_text)\r\n",
        "\r\n",
        "\tdef replace_currency(self, input_text):\r\n",
        "\t\t# Replace remaining currency symbols with 'currsymb'\r\n",
        "\t\treturn re.sub(r'[$|£|€]', ' currsymb ', input_text)\r\n",
        "\r\n",
        "\tdef replace_urls(self, input_text):\r\n",
        "\t\t# Replace links with 'weblink'\r\n",
        "\t\tlink_regex = r'(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)'\r\n",
        "\t\tlink_regex1 = r'https?://\\S+|www\\.\\S+'\r\n",
        "\t\tlink_regex2 = r'http.?://[^\\s]+[\\s]?'\r\n",
        "\t\treturn re.sub(link_regex1, ' weblink ', input_text)\r\n",
        "\r\n",
        "\tdef replace_phone_numbers(self, input_text):\r\n",
        "\t\t# Replace phone numbers with 'phonenumber'\r\n",
        "\t\treturn re.sub(r'\\d{5,}', ' phonenumber ', input_text)\r\n",
        "\r\n",
        "\tdef replace_punctuation(self, input_text):\r\n",
        "\t\t# Replace punctuation with a space\r\n",
        "\t\treturn input_text.translate(str.maketrans(dict.fromkeys(list(string.punctuation),' ')))\r\n",
        "\t\r\n",
        "\tdef remove_extra_whitespace(self, input_text):\r\n",
        "\t\t# Remove extra whitespaces\r\n",
        "\t\treturn re.sub(r'\\s+', ' ', input_text)\r\n",
        "\r\n",
        "\tdef remove_digits(self, input_text):\r\n",
        "\t\treturn re.sub('\\d+', '', input_text)\r\n",
        "\r\n",
        "\tdef remove_stopwords(self, input_text):\r\n",
        "\t\tstop_words = stopwords.words('english')\r\n",
        "\r\n",
        "\t\twords = input_text.split()\r\n",
        "\t\tclean_words = [word for word in words if word not in stop_words]\r\n",
        "\t\treturn ' '.join(clean_words)\r\n",
        "\r\n",
        "\tdef apply_stemming(self, input_text):\r\n",
        "\t\tstemmer = SnowballStemmer('english')\r\n",
        "\t\t\r\n",
        "\t\twords = input_text.split()\r\n",
        "\t\tstemmed_words = [stemmer.stem(word) for word in words]\r\n",
        "\t\treturn ' '.join(stemmed_words)\r\n",
        "\r\n",
        "\r\n",
        "\tdef fit(self, X, y=None, **fit_params):\r\n",
        "\t\treturn self\r\n",
        "\r\n",
        "\tdef transform(self, X, **transform_params):\r\n",
        "\t\tclean_X = X.apply(self.to_lower)\r\n",
        "\t\tclean_X = clean_X.apply(self.replace_brackets)\r\n",
        "\t\tclean_X = clean_X.apply(self.replace_money)\r\n",
        "\t\tclean_X = clean_X.apply(self.replace_currency)\r\n",
        "\t\tclean_X = clean_X.apply(self.replace_urls)\r\n",
        "\t\tclean_X = clean_X.apply(self.replace_phone_numbers)\r\n",
        "\t\tclean_X = clean_X.apply(self.replace_punctuation)\r\n",
        "\t\tclean_X = clean_X.apply(self.remove_extra_whitespace)\r\n",
        "\t\tclean_X = clean_X.apply(self.remove_digits)\r\n",
        "\t\tclean_X = clean_X.apply(self.remove_stopwords)\r\n",
        "\t\tclean_X = clean_X.apply(self.apply_stemming)\r\n",
        "\t\treturn clean_X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f2769bb"
      },
      "source": [
        "One side-effect of text cleaning is that some rows do not have any words left in their text. To deal with these missing values, we impute them with some placeholder text like `no_text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d9f802b",
        "outputId": "75d872e0-34fd-43a6-d92a-d03b9cb9b71b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "ct = CleanText()\n",
        "\n",
        "clean_msg = ct.fit_transform(df.message)\n",
        "empty = clean_msg == ''\n",
        "print(f'{clean_msg[empty].count()} records have no words left after text cleaning')\n",
        "clean_msg.loc[empty] = 'no_text'\n",
        "\n",
        "df['message_clean'] = clean_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "damaged-strength"
      },
      "source": [
        "### Target Encoding (spam = 1 / ham = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "environmental-knock",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "targets = encoder.fit_transform(df.target)\n",
        "\n",
        "df['target_encoded'] = targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14869d38"
      },
      "source": [
        "### Count of spam messages containing phone numbers, weblinks and money"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e42055cd",
        "outputId": "e1937054-2072-427a-dffa-b003daa9fb80"
      },
      "outputs": [],
      "source": [
        "num_phone = len(df.query(\"phone_count!=0 and target_encoded==1\"))\n",
        "num_link = len(df.query(\"links_count!=0 and target_encoded==1\"))\n",
        "num_money = len(df.query(\"money_count!=0 and target_encoded==1\"))\n",
        "num_brackets = len(df.query(\"brackets_count!=0 and target_encoded==1\"))\n",
        "\n",
        "print(f'There are {num_phone} rows containing phone numbers, {num_link} ones containing links, {num_money} ones containing money, and {num_brackets} ones containing brackets.')\n",
        "print(f'out of {len(df[df.target_encoded==1])} spam message')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "broad-couple"
      },
      "source": [
        "## Tokens Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "greater-billy"
      },
      "source": [
        "### Top words in ham messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "engaged-latitude",
        "outputId": "7b67dfbd-be06-4bce-d6c1-5831cf346c45"
      },
      "outputs": [],
      "source": [
        "wc = WordCloud(background_color='white', max_words=150)\n",
        "wc.generate(' '.join(df['message_clean'][df['target']=='ham']))\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Top words in HAM messages',fontdict={'size': 22,  'verticalalignment': 'bottom'})\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pointed-cambodia"
      },
      "source": [
        "### Top words in spam messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "sensitive-housing",
        "outputId": "cb970900-3b8b-4a62-9d52-4cfd6c55c9be"
      },
      "outputs": [],
      "source": [
        "wc = WordCloud(background_color='white', max_words=150)\n",
        "wc.generate(' '.join(df['message_clean'][df['target']=='spam']))\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('Top words in SPAM messages',fontdict={'size': 22,  'verticalalignment': 'bottom'})\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7712fb7"
      },
      "source": [
        "- As we can notice, phone numbers and money amounts are mostly used in spam messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ae2e98d"
      },
      "source": [
        "## Splitting data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1b949a6",
        "outputId": "d0c5b203-b042-48d6-a41b-bc91e9d2692c"
      },
      "outputs": [],
      "source": [
        "X, y = df.drop(['target_encoded','target'], axis=1), df.target_encoded\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=37)\n",
        "\n",
        "print('Train shapes:', X_train.shape, y_train.shape,'\\nTest shapes:', X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c685c1a3"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24458a0c"
      },
      "outputs": [],
      "source": [
        "textcountscols = ['word_count', 'sentence_count', 'brackets_count', 'links_count', 'phone_count', 'money_count']    \r\n",
        "\r\n",
        "def grid_vect(clf, clf_params, X_train, y_train, X_test, y_test, parameters_text=None, vectorizer=None, cv_nb=5):\r\n",
        "    vect_pipe = Pipeline([\r\n",
        "        ('cleantext', ColumnExtractor(cols='message_clean')),\r\n",
        "        ('vect', vectorizer)\r\n",
        "    ])\r\n",
        "\r\n",
        "    col_ext = ColumnExtractor(cols=textcountscols)\r\n",
        "    features = FeatureUnion([('textcounts', col_ext), ('pipe', vect_pipe)], n_jobs=1)\r\n",
        "    \r\n",
        "    pipeline = Pipeline([('features', features), ('clf', clf)])\r\n",
        "\r\n",
        "    # Join both parameters dictionaries together\r\n",
        "    parameters = dict()\r\n",
        "    if parameters_text:\r\n",
        "        parameters.update(parameters_text)\r\n",
        "    parameters.update(clf_params)\r\n",
        "    \r\n",
        "    grid_search = GridSearchCV(pipeline, parameters, scoring='f1', n_jobs=-1, verbose=2, cv=cv_nb)\r\n",
        "    \r\n",
        "    print(\"Performing Grid Search...\")\r\n",
        "    grid_search.fit(X_train, y_train)\r\n",
        "    print(\"Grid Search Results :\")\r\n",
        "    print(f\"Best CV score: {grid_search.best_score_}\")\r\n",
        "    print(\"Best parameters :\")\r\n",
        "\r\n",
        "    best_parameters = grid_search.best_estimator_.get_params()\r\n",
        "    for param_name in sorted(parameters.keys()):\r\n",
        "        print(f\"\\t{param_name}: {best_parameters[param_name]}\")\r\n",
        "\r\n",
        "    GS_best_estimator = grid_search.best_estimator_\r\n",
        "    print(f\"Train score with best_estimator_: {GS_best_estimator.score(X_train, y_train):.3f}\")\r\n",
        "    print(f\"Test score with best_estimator_: {GS_best_estimator.score(X_test, y_test):.3f}\")\r\n",
        "\r\n",
        "    print(\"Classification Report Test Data\")\r\n",
        "    print(classification_report(y_test, GS_best_estimator.predict(X_test), target_names=['ham','spam']))\r\n",
        "    \r\n",
        "    return grid_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ec146d8"
      },
      "outputs": [],
      "source": [
        "# Parameter grid settings for the vectorizers\r\n",
        "parameters_vect = {\r\n",
        "    'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\r\n",
        "    'features__pipe__vect__ngram_range': ((1, 1), (1, 2)),\r\n",
        "    'features__pipe__vect__min_df': (1, 2, 3)\r\n",
        "}\r\n",
        "\r\n",
        "# Parameter grid settings for KNN\r\n",
        "parameters_knn = {\r\n",
        "    'clf__n_neighbors': (3, 5, 7, 8, 9, 10, 25, 30, 50, 70, 74)\r\n",
        "}\r\n",
        "\r\n",
        "# Parameter grid settings for MultinomialNB\r\n",
        "parameters_mnb = {\r\n",
        "    'clf__alpha': (0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.75, 0.8, 0.9, 1.0)\r\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aedd39f"
      },
      "source": [
        "## Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1eca8fb"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier()\r\n",
        "mnb = MultinomialNB()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3f01720"
      },
      "source": [
        "### CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cea03e3"
      },
      "outputs": [],
      "source": [
        "c_vect = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN x CountVectorizer\n",
        "knn_countvect = grid_vect(knn, parameters_knn, X_train, y_train, X_test, y_test, parameters_text=parameters_vect, vectorizer=c_vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a620bff",
        "outputId": "82688949-ce7f-4cb9-e91a-31d63495b2cb"
      },
      "outputs": [],
      "source": [
        "# MultinomialNB x CountVectorizer\n",
        "mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, y_train, X_test, y_test, parameters_text=parameters_vect, vectorizer=c_vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6765f68"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e41a8ce"
      },
      "outputs": [],
      "source": [
        "tfidf_vect = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN x TF-IDF\n",
        "knn_tfidf = grid_vect(knn, parameters_knn, X_train, y_train, X_test, y_test, parameters_text=parameters_vect, vectorizer=tfidf_vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b9d8083",
        "outputId": "6d33fab9-5238-4696-de63-b982b221e1ab"
      },
      "outputs": [],
      "source": [
        "# MultinomialNB x TF-IDF\n",
        "mnb_tfidf = grid_vect(mnb, parameters_mnb, X_train, y_train, X_test, y_test, parameters_text=parameters_vect, vectorizer=tfidf_vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a2de316"
      },
      "source": [
        "## Best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjOToR9kIGBA",
        "outputId": "4f3ecd38-835f-4ab1-a771-46f0c20a0b5c"
      },
      "outputs": [],
      "source": [
        "textcountscols = ['word_count', 'sentence_count', 'brackets_count', 'links_count', 'phone_count', 'money_count']    \n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.25, min_df=1, ngram_range=(1,2))\n",
        "clf_mnb = MultinomialNB(alpha=0.3) \n",
        "\n",
        "vect_pipe = Pipeline([('cleantext', ColumnExtractor(cols='message_clean')),('vect', vectorizer)])\n",
        "col_ext = ColumnExtractor(cols=textcountscols)\n",
        "\n",
        "features = FeatureUnion([('textcounts', col_ext), ('pipe', vect_pipe)], n_jobs=-1)\n",
        "pipeline = Pipeline([('features', features), ('clf', clf_mnb)])\n",
        "\n",
        "best_model = pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = '../model/model.joblib'\n",
        "dump(best_model, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recall = cross_val_score(best_model, X_test, y_test, cv=10, scoring='recall')\n",
        "print(f'Recall : Mean = {np.mean(recall)}')\n",
        "print(recall)\n",
        "\n",
        "print('-'*10)\n",
        "\n",
        "precision = cross_val_score(best_model, X_test, y_test, cv=10, scoring='precision')\n",
        "print(f'Precision : Mean = {np.mean(precision)}')\n",
        "print(precision)\n",
        "\n",
        "print('-'*10)\n",
        "\n",
        "f1 = cross_val_score(best_model, X_test, y_test, cv=10, scoring='f1')\n",
        "print(f'f1 : Mean = {np.mean(f1)}')\n",
        "print(f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model_path, data):\n",
        "    df = pd.DataFrame(data, columns=['message'])\n",
        "    tc = FeaturesExtractor()\n",
        "    ct = CleanText()\n",
        "\n",
        "    df_counts = tc.transform(df)\n",
        "    df_clean = ct.transform(df.message)\n",
        "    df = df_counts\n",
        "    df['message_clean'] = df_clean\n",
        "    \n",
        "    model = load(model_path)\n",
        "    predictions = model.predict(df).tolist()\n",
        "    predictions = [ ['ham','spam'][prediction] for prediction in predictions]\n",
        "    print('Model predictions:')\n",
        "    for i in range(len(data)):\n",
        "        print(f'{data[i]} : {predictions[i]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Flasnf4hMeEp",
        "outputId": "5879bada-e4c9-4745-b48d-b23097ef050a"
      },
      "outputs": [],
      "source": [
        "my_msg = [\"Hello, can you please call me later ?\",\n",
        "          \"Lets meet 2morroww after class\",\n",
        "          \"Congratulations! You've won a $1OOO Walmart gift card. Go to http://bit.ly/123456to claim now.\",\n",
        "          \"Take 20% off your order with code THANKYOU.\"]\n",
        "\n",
        "predict(model_path, my_msg)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "f651b3f7",
        "dc8226c2",
        "72c1acfd",
        "damaged-strength",
        "14869d38",
        "greater-billy",
        "pointed-cambodia",
        "7ae2e98d",
        "12c9527d",
        "698f5590",
        "8c84909f",
        "b66a657f"
      ],
      "name": "spam_ham.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "1b85e8219f92fedc24155e30ae00b2d95423657aecea3a72ddb219570703f90a"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit ('AI-ROSAfgDd': venv)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "metadata": {
      "interpreter": {
        "hash": "92101890d71806977023fa7b2b15eb79a22bc7ce09426790cb18f9a1f6b1af3b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}